SHREE SWAMI SAMARTH: 
*----------------------------------------------*-------------------------------------------------*
INSTALL KUBEADM ON ALL NDOES: 

sudo apt-get update
sudo apt-get install -y apt-transport-https ca-certificates curl gpg


sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.33/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

Add APT REPO: 

echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.33/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list

sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl

*----------------------------------------------*-------------------------------------------------*

INSTALL CONTAINER D: On All machines. And settings to be done on all machines. 

sudo apt-get update
sudo apt-get install -y containerd

CHECK FOR THE CGROUPS: 

ps -p 1 # gets the current systemd. This means we will usr systemcd cgroups driver for controlling resources for containerd. 

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml # change this: SystemdCgroup = false to SystemdCgroup = true

sudo containerd config default | sed 's/SystemdCgroup = false/SystemdCgroup = true/' | sudo tee /etc/containerd/config.toml

cat /etc/containerd/config.toml | grep SystemdCgroup -B 50    # confirm this:  and it should be in thr right group 

OUTPUT: 
  
          [plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
            BinaryName = ""
            CriuImagePath = ""
            CriuPath = ""
            CriuWorkPath = ""
            IoGid = 0
            IoUid = 0
            NoNewKeyring = false
            NoPivotRoot = false
            Root = ""
            ShimCgroup = ""
            SystemdCgroup = true

Restart ContainerD: 

 sudo systemctl restart containerd
 sudo systemctl status containerd


*----------------------------------------------*-------------------------------------------------*

Initiate kubeadm: 

IP Route: 

   ip route show
    default via 10.50.0.1 dev ens160 proto static
    10.50.0.0/20 dev ens160 proto kernel scope link src 10.50.4.78

*----------------------------------------------*-------------------------------------------------*


Set the IP Filter: This must be done on all nodes. 


cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

sudo sysctl --system # apply configuration without reboot: 


*----------------------------------------------*-------------------------------------------------*

KUBEADM INITIALIZATION COMMAND: 

sudo kubeadm init --apiserver-advertise-address  10.50.4.78 --pod-network-cidr "10.244.0.0/16" --upload-certs

Config the Directory: 
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

kubectl get nodes

*----------------------------------------------*-------------------------------------------------*

Setup CNI: 

Network addon is required for POD network setup:
We are using the Flannel: So get the file from here. 
  https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

-- The below section specifies the CIDR block for POD networking. Here, the network must be the same as the CIDR from the --pod-network-cidr "10.244.0.0/16" KUBEADM INIT command. 
      net-conf.json: |
          {
            "Network": "10.244.0.0/16",
            "EnableNFTables": false,
            "Backend": {
              "Type": "vxlan"
            }
          }




*----------------------------------------------*-------------------------------------------------*

Open Ports: On all machines

sudo ufw allow 6443/tcp
sudo ufw allow 2379:2380/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 30000:32767/tcp # For NodePort services

Join the worker nodes to master from the command. 




